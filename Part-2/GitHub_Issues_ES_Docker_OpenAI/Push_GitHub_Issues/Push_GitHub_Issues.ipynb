{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c8a7cc-de53-4725-8ae6-8c128e581b8b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "    \n",
    "### <center> GITHUB ISSUES</center>\n",
    "### <center> ELASTICSEARCH - OPEN AI</center>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "<br>\n",
    "    <br>\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffddc3a-6993-4848-bea0-6e392ae30da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\anaconda\\lib\\site-packages (1.54.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\anaconda\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\anaconda\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\anaconda\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\anaconda\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\anaconda\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\anaconda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c32c3-fdb5-4391-800f-38864eaaae01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8da7f1d-cc25-452e-9aa1-f019edfd90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in d:\\anaconda\\lib\\site-packages (8.15.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in d:\\anaconda\\lib\\site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in d:\\anaconda\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.2.3)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#Install elastic search\n",
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79588c-116b-4bc7-86df-e6ce16ce6f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7aee99d-0219-4469-9f8a-d1c32463ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import requests\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d15e9-bc44-4510-b3c5-40632ebc8299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13b0610-c0c9-4e04-9068-d332231b904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the headers\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"access_token\": \"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\",\n",
    "    \"Git_Username\":\"NFA24SCM76P\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f3b1c-0863-40a0-b3e0-40e7b26ce0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce5822-6d66-49b7-9523-ba0c3a074da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f6f4a9c-94f6-43d5-8d64-e5521cea24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the owners and repositories\n",
    "owners = ['langchain-ai', 'langchain-ai', 'microsoft', 'openai', 'elastic', 'milvus-io']\n",
    "repos = ['langchain', 'langgraph', 'autogen', 'openai-cookbook', 'elasticsearch', 'pymilvus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "decd06a3-5c46-4048-a22b-180bde820141",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "per_page = 100\n",
    "from_date = (dt.date.today() - dt.timedelta(days=60)).isoformat() #The duration for which we need the issues can be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0c0bc88-58d5-43ff-ad3c-9a8d3cb71e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(owner, repo):\n",
    "    return f\"https://api.github.com/repos/{owner}/{repo}/issues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515dc9c3-429f-4172-a3a9-0d9ba5e96752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total issues fetched: 6355\n"
     ]
    }
   ],
   "source": [
    "# List to store issues\n",
    "issues = []\n",
    "\n",
    "# Iterate over each owner and repository pair\n",
    "for owner, repo in zip(owners, repos):\n",
    "    page = 1\n",
    "    flag = True\n",
    "    url = fetch_url(owner, repo)\n",
    "\n",
    "    while flag:\n",
    "        response = requests.get(url, headers=headers, params={\"since\": from_date, \"page\": page, \"state\": \"all\", \"per_page\": 100})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            for obj in data:\n",
    "                created_at = datetime.strptime(obj[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                if created_at >= datetime.strptime(from_date, \"%Y-%m-%d\"):\n",
    "                    issues.append({\n",
    "                        \"_type\": \"issue\",\n",
    "                        \"_repo\": repo,\n",
    "                        \"_issueNumber\": str(obj['number']),\n",
    "                        \"_title\": str(obj['title']),\n",
    "                        \"_createdAt\": str(obj['created_at']),\n",
    "                        \"_closedAt\": str(obj['closed_at']) if obj['closed_at'] else \"2024-12-31T00:36:30Z\",\n",
    "                        \"_state\": str(obj['state']),\n",
    "                        \"_body\": str(obj['body'])[:5000]  # Truncate body to 5000 characters\n",
    "                    })\n",
    "                else:\n",
    "                    flag = False\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Failed to fetch issues from {repo}: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "\n",
    "# Output the total number of issues fetched\n",
    "print(f\"Total issues fetched: {len(issues)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7843a0f6-914f-417f-9733-179d3787498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_body': '### URL\\n'\n",
      "          '\\n'\n",
      "          '_No response_\\n'\n",
      "          '\\n'\n",
      "          '### Checklist\\n'\n",
      "          '\\n'\n",
      "          '- [X] I added a very descriptive title to this issue.\\n'\n",
      "          '- [X] I included a link to the documentation page I am referring to '\n",
      "          '(if applicable).\\n'\n",
      "          '\\n'\n",
      "          '### Issue with current documentation:\\n'\n",
      "          '\\n'\n",
      "          'I need to know on the maximum chunk size that can be return from '\n",
      "          'SemanticChunker.split_documents() for large documents.\\r\\n'\n",
      "          'Can it be more than 8k token as i need to send the chunk for '\n",
      "          'embedding, chunk with more than 8k token will fail with Azure '\n",
      "          'embedding model.\\r\\n'\n",
      "          'Need Help!!\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '### Idea or request for content:\\n'\n",
      "          '\\n'\n",
      "          'Documentation should cleary explain this',\n",
      " '_closedAt': '2024-12-31T00:36:30Z',\n",
      " '_createdAt': '2024-11-21T06:50:49Z',\n",
      " '_issueNumber': '28250',\n",
      " '_repo': 'langchain',\n",
      " '_state': 'open',\n",
      " '_title': 'DOC: What is the maximum chunk size returned from '\n",
      "           'SemanticChunker.split_documents()',\n",
      " '_type': 'issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Sample Issue\n",
    "pprint(issues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e6c3e7d-032d-414c-88af-db7d82cd4be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6355\n"
     ]
    }
   ],
   "source": [
    "#Number of Issues in the given timeframe\n",
    "pprint(len(issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68599a46-1c6b-4897-a4e6-190059999f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of Issues to a DataFrame\n",
    "df_Issues = pd.DataFrame(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a0a26fa-5fce-4e10-b2b1-5011a43d733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all NaN values with None in columns as elasticsearch does not recognize it\n",
    "df_Issues.fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0ce3ebd-bbd5-4edc-a5ab-ca1ef4c6779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embeddings from OpenAI API\n",
    "def embed(texts):\n",
    "    # Make a request to OpenAI API to get embeddings\n",
    "    embeddings = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model='text-embedding-ada-002'\n",
    "    )\n",
    "    # Extract embeddings from the API response\n",
    "    return [result.embedding for result in embeddings.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89df7bc0-1fd5-485b-a9c2-29283dfe7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/6355 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                        | 500/6355 [01:03<12:21,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▎                                                                 | 1000/6355 [02:08<11:30,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████▍                                                           | 1500/6355 [03:13<10:26,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████▌                                                     | 2000/6355 [04:18<09:24,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████▋                                               | 2500/6355 [05:21<08:14,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████▊                                         | 3000/6355 [06:25<07:10,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████▉                                   | 3500/6355 [07:28<06:04,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████                             | 4000/6355 [08:32<05:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████████████████▏                      | 4500/6355 [09:35<03:56,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████▎                | 5000/6355 [10:39<02:52,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████▌          | 5500/6355 [11:43<01:49,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6355/6355 [12:48<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n"
     ]
    }
   ],
   "source": [
    "## Embedding creation using openAI of GitHub Issues.\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "Issue_embeddings = []\n",
    "\n",
    "# Batch size for processing data\n",
    "batch_size = 500\n",
    "\n",
    "# Initialize data structure for storing text\n",
    "data = [\n",
    "    [], # Titles\n",
    "]\n",
    "count=0;\n",
    "# Embed and insert in batches\n",
    "for i in tqdm(range(0, len(df_Issues))):\n",
    "    title = str(df_Issues.iloc[i]['_title']).replace(\"\\n\", \"\") or ''\n",
    "    body = str(df_Issues.iloc[i]['_body']).replace(\"\\n\", \"\") or ''\n",
    "    \n",
    "    # Merge 'repository name','title' and 'body' of the GitHub Issue\n",
    "    combined_text = f\"Repository:{owner}/{repo} Issue Title:{title} Issue Body:{body}\"  \n",
    "    data[0].append(combined_text)\n",
    "    if len(data[0]) % batch_size == 0:\n",
    "        print(\"Embedding batch...\")\n",
    "\n",
    "        embeddings_batch = embed(data[0]) \n",
    "        Issue_embeddings.extend(embeddings_batch)\n",
    "        data = [[]]\n",
    "        print(\"Waiting for 1 minute before the next batch...\")\n",
    "        \n",
    "        time.sleep(60)    \n",
    "        \n",
    "# Embed the remaining data if any\n",
    "if len(data[0]) != 0:\n",
    "    embeddings_rem = embed(data[0])\n",
    "    print(len(embeddings_rem))\n",
    "    Issue_embeddings.extend(embeddings_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dacbf-ef0a-4f7c-b13f-427c37113eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a630d2f2-346a-4927-8bd6-761b097f09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Generated embeddings to GitHub_Issue_vector column in the dataframe\n",
    "\n",
    "df_Issues[\"GitHub_Issue_vector\"] = Issue_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca323fed-13c1-4636-bebb-dd7c3257e5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a3c634e-a209-4f00-9ed5-59bc16ad45df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>_repo</th>\n",
       "      <th>_issueNumber</th>\n",
       "      <th>_title</th>\n",
       "      <th>_createdAt</th>\n",
       "      <th>_closedAt</th>\n",
       "      <th>_state</th>\n",
       "      <th>_body</th>\n",
       "      <th>GitHub_Issue_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6350</th>\n",
       "      <td>issue</td>\n",
       "      <td>pymilvus</td>\n",
       "      <td>2274</td>\n",
       "      <td>[Bug]: Missing attributes when using LocalBulk...</td>\n",
       "      <td>2024-09-25T09:35:08Z</td>\n",
       "      <td>2024-11-19T08:52:34Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>### Is there an existing issue for this?\\r\\n\\r...</td>\n",
       "      <td>[-0.019479956477880478, 0.012543493881821632, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6351</th>\n",
       "      <td>issue</td>\n",
       "      <td>pymilvus</td>\n",
       "      <td>2273</td>\n",
       "      <td>remove analyzer_params, added enable_tokenizer...</td>\n",
       "      <td>2024-09-25T02:34:00Z</td>\n",
       "      <td>2024-10-08T03:53:20Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>the API for `enable_match` has changed to: onl...</td>\n",
       "      <td>[-0.011820919811725616, -0.009375212714076042,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>issue</td>\n",
       "      <td>pymilvus</td>\n",
       "      <td>2272</td>\n",
       "      <td>fix: not put 'default_value' in dict</td>\n",
       "      <td>2024-09-24T12:07:18Z</td>\n",
       "      <td>2024-10-08T02:19:16Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>https://github.com/milvus-io/milvus/issues/36457</td>\n",
       "      <td>[-0.025580786168575287, -0.003847957355901599,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6353</th>\n",
       "      <td>issue</td>\n",
       "      <td>pymilvus</td>\n",
       "      <td>2271</td>\n",
       "      <td>[Bug]: AmbiguousIndexName when create local ve...</td>\n",
       "      <td>2024-09-24T01:41:48Z</td>\n",
       "      <td>2024-12-31T00:36:30Z</td>\n",
       "      <td>open</td>\n",
       "      <td>### Is there an existing issue for this?\\n\\n- ...</td>\n",
       "      <td>[-0.01964668184518814, 0.01618204638361931, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6354</th>\n",
       "      <td>issue</td>\n",
       "      <td>pymilvus</td>\n",
       "      <td>2270</td>\n",
       "      <td>[QUESTION]: When I use multiple threads to req...</td>\n",
       "      <td>2024-09-23T02:23:42Z</td>\n",
       "      <td>2024-12-31T00:36:30Z</td>\n",
       "      <td>open</td>\n",
       "      <td>### Is there an existing issue for this?\\n\\n- ...</td>\n",
       "      <td>[-0.016197411343455315, 0.00210845610126853, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _type     _repo _issueNumber  \\\n",
       "6350  issue  pymilvus         2274   \n",
       "6351  issue  pymilvus         2273   \n",
       "6352  issue  pymilvus         2272   \n",
       "6353  issue  pymilvus         2271   \n",
       "6354  issue  pymilvus         2270   \n",
       "\n",
       "                                                 _title            _createdAt  \\\n",
       "6350  [Bug]: Missing attributes when using LocalBulk...  2024-09-25T09:35:08Z   \n",
       "6351  remove analyzer_params, added enable_tokenizer...  2024-09-25T02:34:00Z   \n",
       "6352               fix: not put 'default_value' in dict  2024-09-24T12:07:18Z   \n",
       "6353  [Bug]: AmbiguousIndexName when create local ve...  2024-09-24T01:41:48Z   \n",
       "6354  [QUESTION]: When I use multiple threads to req...  2024-09-23T02:23:42Z   \n",
       "\n",
       "                 _closedAt  _state  \\\n",
       "6350  2024-11-19T08:52:34Z  closed   \n",
       "6351  2024-10-08T03:53:20Z  closed   \n",
       "6352  2024-10-08T02:19:16Z  closed   \n",
       "6353  2024-12-31T00:36:30Z    open   \n",
       "6354  2024-12-31T00:36:30Z    open   \n",
       "\n",
       "                                                  _body  \\\n",
       "6350  ### Is there an existing issue for this?\\r\\n\\r...   \n",
       "6351  the API for `enable_match` has changed to: onl...   \n",
       "6352   https://github.com/milvus-io/milvus/issues/36457   \n",
       "6353  ### Is there an existing issue for this?\\n\\n- ...   \n",
       "6354  ### Is there an existing issue for this?\\n\\n- ...   \n",
       "\n",
       "                                    GitHub_Issue_vector  \n",
       "6350  [-0.019479956477880478, 0.012543493881821632, ...  \n",
       "6351  [-0.011820919811725616, -0.009375212714076042,...  \n",
       "6352  [-0.025580786168575287, -0.003847957355901599,...  \n",
       "6353  [-0.01964668184518814, 0.01618204638361931, -0...  \n",
       "6354  [-0.016197411343455315, 0.00210845610126853, 0...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the new Column is created\n",
    "df_Issues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682465d1-0413-4e65-8f0d-62a1579bb3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95e897aa-f073-4e35-9c2f-645ae5fdee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Elasticsearch connection\n",
    "from elasticsearch import Elasticsearch,helpers\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "es.ping()   #connection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8622726-a3cf-4a62-a0d9-8d1d57511de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2118a2e8-5c50-49b0-aec5-882a76f606e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'github_issues'})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index Mapping for githubissues\n",
    "\n",
    "index_mapping= {\n",
    "    \"properties\": {\n",
    "      \"GitHub_Issue_vector\": {\n",
    "          \"type\": \"dense_vector\",\n",
    "          \"dims\": 1536,\n",
    "          \"index\": \"true\",\n",
    "          \"similarity\": \"cosine\"\n",
    "      },\n",
    "     \"_type\": {\"type\": \"text\"}, \n",
    "     \"_repo\":{\"type\":\"text\"},   \n",
    "     \"_issueNumber\": {\"type\": \"long\"},    \n",
    "     \"_title\": {\"type\": \"text\"},\n",
    "     \"_createdAt\": {\"type\": \"date\"},\n",
    "     \"_closedAt\": {\"type\": \"date\"},\n",
    "     \"_state\": {\"type\": \"text\"},\n",
    "     \"_body\": {\"type\": \"text\"}\n",
    "   }\n",
    "}\n",
    "\n",
    "if es.indices.exists(index=\"github_issues\"):\n",
    "    es.indices.delete(index=\"github_issues\")\n",
    "\n",
    "es.indices.create(index=\"github_issues\", body={\"mappings\": index_mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dca088-74bc-4d7f-914f-a1d6d382a86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9aaa748c-0660-46ae-a0cc-9571ba1c5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 6355 records into Elasticsearch. Failed records: []\n"
     ]
    }
   ],
   "source": [
    "# Bulk indexing for githubissues\n",
    "\n",
    "def dataframe_to_bulk_actions(df_Issues):\n",
    "    for index, row in df_Issues.iterrows():\n",
    "        yield {\n",
    "            \"_index\": 'github_issues',\n",
    "            \"_source\": {\n",
    "                \"_type\": row['_type'],\n",
    "                \"_repo\":row['_repo'],\n",
    "                \"_issueNumber\": row['_issueNumber'],\n",
    "                \"_title\": row['_title'],\n",
    "                \"_createdAt\": row['_createdAt'],\n",
    "                \"_closedAt\": row['_closedAt'],\n",
    "                \"_state\": row['_state'],\n",
    "                \"_body\": row['_body'],\n",
    "                \"GitHub_Issue_vector\": row['GitHub_Issue_vector']\n",
    "            }\n",
    "        }\n",
    "\n",
    "start = 0\n",
    "end = len(df_Issues)\n",
    "batch_size = 500\n",
    "\n",
    "for batch_start in range(start, end, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, end)\n",
    "    batch_dataframe = df_Issues.iloc[batch_start:batch_end]\n",
    "    actions = list(dataframe_to_bulk_actions(df_Issues.iloc[start:end]))\n",
    "    \n",
    "success, failed = helpers.bulk(es, actions)\n",
    "print(f\"Inserted {success} records into Elasticsearch. Failed records: {failed}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac75e1-947c-40c7-bc53-d6159b9becaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
